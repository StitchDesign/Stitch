//
//  LLMActionRecording.swift
//  Stitch
//
//  Created by Christian J Clampitt on 6/27/24.
//

import Foundation
import StitchSchemaKit


// MARK: recording user's actions in app as a JSON of LLM Step Actions, assigning a natural language prompt and writing JSON + prompt to file

let LLM_START_RECORDING_SF_SYMBOL = "inset.filled.rectangle.badge.record"
let LLM_STOP_RECORDING_SF_SYMBOL = "stop.fill"

// TODO: separate buttons (icons and actions) for generating training data vs correcting what the LLM just sent to us
struct LLMRecordingToggled: StitchDocumentEvent {
    
    func handle(state: StitchDocumentViewModel) {
        
        if state.llmRecording.isRecording {
            let modeLabel = state.llmRecording.mode == .augmentation ? "AUGMENTATION" : "NORMAL"
            log("ğŸ“¼ ğŸ›‘ STOPPING LLM RECORDING MODE [\(modeLabel)] ğŸ›‘ ğŸ“¼")
            state.llmRecordingEnded()
        } else {
            // If we're not already recording, and we're in AI Mode,
            // then start augmentation mode
            let wasInAIMode = state.insertNodeMenuState.isFromAIGeneration
            if wasInAIMode {
                state.startLLMAugmentationMode()
            } else {
                state.llmRecordingStarted()
            }
            
            let modeLabel = state.llmRecording.mode == .augmentation ? "AUGMENTATION" : "NORMAL"
            let transitionNote = wasInAIMode ? " (Transitioned from AI Generation)" : ""
            log("ğŸ“¼ â–¶\u{fef} STARTING LLM RECORDING MODE [\(modeLabel)]\(transitionNote) â–¶\u{fef} ğŸ“¼")
        }
    }
}

extension StitchDocumentViewModel {

    @MainActor
    func startLLMAugmentationMode() {
        
        log("ğŸ”„ ğŸ¤– TRANSITIONING FROM AI MODE TO RECORDING - ENTERING AUGMENTATION MODE ğŸ¤– ğŸ”„")
        
        
        // TODO: these logs are telling us that self.llmRecording.actions is empty (we're not sure how or where they were made empty?); can we populate the actions again, before we open the "edit before submit" modal ?
        
        let derivedActions = self.deriveNewAIActions()
        self.llmRecording.actions = derivedActions
        
        // First store the current AI-generated actions
        log("ğŸ¤– ğŸ’¾ Storing AI-Generated Actions: \(self.llmRecording.actions)")
        
        // Invalidate the StitchAI tip -- don't need to show it to the user again
        self.stitchAITrainingTip.invalidate(reason: .actionPerformed)
        StitchAITrainingTip.hasCompletedOpenAIRequest = false
        
        // Set augmentation mode
        self.llmRecording.mode = .augmentation
        
        // Open the Edit-before-submit modal
        self.llmRecording.modal = .editBeforeSubmit
        
        // Clear the AI generation flag AFTER we've secured the actions
        self.insertNodeMenuState.isFromAIGeneration = false
        log("ğŸ”„ ğŸ¤– AI Generation Mode Cleared - Actions Preserved for Correction ğŸ¤– ğŸ”„")
        
        // Start recording
        self.llmRecordingStarted()
    }
}

/// What we write to JSON/JSONL file
struct LLMRecordingData: Equatable, Encodable {
    let actions: LLMStepActions
    let prompt: String
}

extension StitchDocumentViewModel {
    
    @MainActor
    func llmRecordingStarted() {
        print("ğŸ“¼ âš¡ï¸ LLM Recording Started - isRecording set to true âš¡ï¸ ğŸ“¼")
        print("ğŸ¯ Current Recording Mode: \(self.llmRecording.mode)")
        
        // Debug print current actions before starting recording
//        print("ğŸ¤– Current Actions at Recording Start: \(self.llmRecording.actions.asJSONDisplay())")
        
        self.llmRecording.isRecording = true
        
        // Save initial graph entity state for tracking changes
        // MARK: only overwrite if not yet set, since AI request may have already set it
        self.llmRecording.initialGraphState = self.llmRecording.initialGraphState ?? self.visibleGraph.createSchema()
    }
    
    @MainActor
    func llmRecordingEnded() {
        let currentMode = self.llmRecording.mode
        print("ğŸ“¼ âš¡\u{fef} LLM Recording Ended - isRecording set to false âš¡\u{fef} ğŸ“¼")
        print("ğŸ¯ Current Recording Mode: \(currentMode)")
        self.llmRecording.isRecording = false
        
        // Debug print all actions
//        print("ğŸ¤– Complete Action Sequence: \(self.llmRecording.actions.asJSONDisplay())")
        
        // Cache the json of all actions
        self.llmRecording.promptState.actionsAsDisplayString = self.llmRecording.actions.asJSONDisplay()
        
        // If we stopped recording and have LLMActions
        if !self.llmRecording.actions.isEmpty {
            if currentMode == .augmentation {
                print("ğŸ“¼ ğŸ¤– Augmentation mode - Skipping prompt modal and proceeding to save ğŸ¤– ğŸ“¼")
                self.closedLLMRecordingPrompt()
            } else if !self.llmRecording.hasShownModalInNormalMode {
                print("ğŸ“¼ ğŸ“ Opening LLM Recording Prompt Modal ğŸ“ ğŸ“¼")
                self.llmRecording.promptState.showModal = true
                self.llmRecording.hasShownModalInNormalMode = true
                self.reduxFocusedField = .llmRecordingModal
            }
        }
    }
    
    @MainActor func closedLLMRecordingPrompt() {
        let currentMode = self.llmRecording.mode
        log("ğŸ“¼ ğŸ’¾ Closing LLM Recording Prompt - Saving Data ğŸ’¾ ğŸ“¼")
        log("ğŸ¯ Current Mode for Upload: \(currentMode)")
        
        self.llmRecording.promptState.showModal = false
        self.reduxFocusedField = nil
        
        let actions = self.llmRecording.actions
        
        guard !actions.isEmpty else {
            log("ğŸ“¼ âš ï¸ No actions to save - Resetting recording state âš ï¸ ğŸ“¼")
            self.llmRecording = .init()
            return
        }
        
        self.showLLMEditModal()
    }
}
